# Aphasia in large language models

Scientists learned a lot about the structure and function of the brain by studying patients with brain injuries - e.g. https://en.wikipedia.org/wiki/Broca%27s_area. I'm performing a similar analysis for LLMs. This repo contains my analysis of the BERT model. Specifically I'm researching - what happens to the model when we start replacing model weights with 0's? Can we use fine tuning to intentionally remove capabilities from the model?

I'm writing about my analysis on [substack](https://indiequant.substack.com/p/building-b0rt).


