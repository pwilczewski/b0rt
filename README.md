# Aphasia in large language models

Scientists learned a lot about the structure and function of the brain by studying patients with brain injuries - e.g. [Broca's area](https://en.wikipedia.org/wiki/Broca%27s_area). I am performing similar research for LLMs. This repo contains my analysis of the BERT model. Specifically I'm researching - what happens to the model when we start replacing model weights with 0's? Can we use fine tuning to intentionally remove capabilities from the model?

I am publishing my results on [substack](https://indiequant.substack.com/p/building-b0rt).
