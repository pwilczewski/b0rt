# Inducing aphasia in large language models

Scientists learned a lot about the structure and function of the brain by studying patients with brain injuries - e.g. [Broca's area](https://en.wikipedia.org/wiki/Broca%27s_area). My hypothesis is that similar research can help us better understand LLMs. This repo contains my analysis of the BERT model. Specifically I am researching - what happens to the model's capabilities when I start replacing model weights with 0's? Can I use fine tuning to intentionally remove capabilities from the model?

I am writing about this research on [substack](https://indiequant.substack.com/p/building-b0rt).
