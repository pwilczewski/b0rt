{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\anaconda3\\envs\\b0rt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked titles and answers for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_answers = [\"witch\",\"pride\",\"kill\",\"wrath\",\"young\",\"hundred\",\"monte\",\"search\",\n",
    "                \"war\",\"punishment\",\"rises\",\"madame\",\"heights\",\"being\",\"streetcar\",\n",
    "                \"glass\",\"rex\",\"much\",\"nest\",\"lord\",\"catcher\",\"adventures\",\n",
    "                \"chocolate\",\"cities\"]\n",
    "\n",
    "masked_titles = [\"the lion the [MASK] and the wardrobe.\",\n",
    "\"[MASK] and prejudice.\",\n",
    "\"to [MASK] a mockingbird.\",\n",
    "\"the grapes of [MASK].\",\n",
    "\"a portrait of an artist as a [MASK] man.\",\n",
    "\"one [MASK] years of solitude.\",\n",
    "\"the count of [MASK] cristo.\",\n",
    "\"in [MASK] of lost time.\",\n",
    "\"[MASK] and peace.\",\n",
    "\"crime and [MASK].\",\n",
    "\"the sun also [MASK].\",\n",
    "\"[MASK] bovary.\",\n",
    "\"wuthering [MASK].\",\n",
    "\"the importance of [MASK] earnest.\",\n",
    "\"a [MASK] named desire.\",\n",
    "\"the [MASK] menagerie.\",\n",
    "\"oedipus [MASK].\",\n",
    "\"[MASK] ado about nothing.\",\n",
    "\"one flew over the cuckoo's [MASK].\",\n",
    "\"the [MASK] of the rings.\",\n",
    "\"the [MASK] in the rye.\",\n",
    "\"alice's [MASK] in wonderland.\",\n",
    "\"charlie and the [MASK] factory.\",\n",
    "\"a tale of two [MASK].\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code for evaluating fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate fine tuned model against original predictions\n",
    "def evaluate_fine_tuning(ft_model_str, eval_titles, eval_answers):\n",
    "\n",
    "    model = BertForMaskedLM.from_pretrained(ft_model_str)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert_orig')\n",
    "    plz_orig = pipeline('fill-mask', model='bert_orig')\n",
    "    plz_ft = pipeline('fill-mask', model=ft_model_str)\n",
    "\n",
    "    results = []\n",
    "    for title, answer in zip(eval_titles, eval_answers):\n",
    "\n",
    "        # bert original answer\n",
    "        top_tokens_orig = plz_orig(title)\n",
    "        top_answer_orig = top_tokens_orig[0]['token_str']\n",
    "        prob_answer_orig = top_tokens_orig[0]['score']\n",
    "\n",
    "        # bert fine tuned answer\n",
    "        top_tokens_tuned = plz_ft(title)\n",
    "        top_answer_tuned = top_tokens_tuned[0]['token_str']\n",
    "        prob_answer_tuned = top_tokens_tuned[0]['score']\n",
    "\n",
    "        # compute probability of original answer\n",
    "        title_tokens = tokenizer(title, return_tensors='pt')\n",
    "        model_logits = model(**title_tokens, return_dict=True).logits\n",
    "        mask_index = title_tokens['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "        answer_id = tokenizer.convert_tokens_to_ids(answer)\n",
    "        pr_orig_answer = torch.softmax(model_logits, dim=-1)\n",
    "        pr_orig_answer = pr_orig_answer[0, mask_index, answer_id].item()\n",
    "\n",
    "        results.append([title, top_answer_orig, prob_answer_orig, pr_orig_answer, top_answer_tuned, prob_answer_tuned])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['title', 'orig_answer', 'orig_score', 'tuned_score', 'tuned_answer', 'tuned_answer_score'])\n",
    "    return results_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate models on titles used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>d_score</th>\n",
       "      <th>disaccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_embeddings_finetuned</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert_attention_layer0_finetuned</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>79.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert_attention_layer5_finetuned</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>95.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert_attention_layer11_finetuned</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert_classification_finetuned</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model  d_score  disaccuracy\n",
       "0         bert_embeddings_finetuned    -0.71       100.00\n",
       "1   bert_attention_layer0_finetuned    -0.67        79.17\n",
       "2   bert_attention_layer5_finetuned    -0.69        95.83\n",
       "3  bert_attention_layer11_finetuned    -0.72       100.00\n",
       "4     bert_classification_finetuned    -0.67        87.50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through fine tuned models and evaluate results\n",
    "model_list = ['bert_embeddings_finetuned','bert_attention_layer0_finetuned','bert_attention_layer5_finetuned','bert_attention_layer11_finetuned','bert_classification_finetuned']\n",
    "\n",
    "model_evals = []\n",
    "for model in model_list:\n",
    "    eval_results_train = evaluate_fine_tuning(model, masked_titles, mask_answers)\n",
    "    disaccuracy = 100*(1-np.mean(eval_results_train['orig_answer'] == eval_results_train['tuned_answer']))\n",
    "    d_score = np.mean(eval_results_train['tuned_score'] - eval_results_train['orig_score'])\n",
    "    model_evals.append([model, round(d_score,2), round(disaccuracy,2)])\n",
    "\n",
    "pd.DataFrame(model_evals, columns=[\"model\",\"d_score\",\"disaccuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate model on titles not used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>d_score</th>\n",
       "      <th>disaccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_embeddings_finetuned</td>\n",
       "      <td>0.03</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert_attention_layer0_finetuned</td>\n",
       "      <td>0.03</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert_attention_layer5_finetuned</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert_attention_layer11_finetuned</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert_classification_finetuned</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model  d_score  disaccuracy\n",
       "0         bert_embeddings_finetuned     0.03         10.0\n",
       "1   bert_attention_layer0_finetuned     0.03         10.0\n",
       "2   bert_attention_layer5_finetuned    -0.10         30.0\n",
       "3  bert_attention_layer11_finetuned    -0.14         50.0\n",
       "4     bert_classification_finetuned    -0.02          0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that my fine tuning doesn't impact other masks\n",
    "test_sample_answers = [\"species\", \"human\", \"road\", \"reason\", \"good\", \"double\", \"man\", \"atomic\", \"war\", \"roman\"]\n",
    "test_sample_titles = [\"on the origin of [MASK].\",\n",
    "                      \"an essay concerning [MASK] understanding.\",\n",
    "                      \"the [MASK] to serfdom.\",\n",
    "                      \"critique of pure [MASK].\",\n",
    "                      \"beyond [MASK] and evil.\",\n",
    "                      \"the [MASK] helix.\",\n",
    "                      \"the nature and destiny of [MASK].\",\n",
    "                      \"the making of the [MASK] bomb.\",\n",
    "                      \"the second world [MASK].\",\n",
    "                      \"the rise and fall of the [MASK] empire.\"]\n",
    "\n",
    "model_evals = []\n",
    "for model in model_list:\n",
    "    eval_results_test = evaluate_fine_tuning(model, test_sample_titles, test_sample_answers)\n",
    "    disaccuracy = 100*(1-np.mean(eval_results_test['orig_answer'] == eval_results_test['tuned_answer']))\n",
    "    d_score = np.mean(eval_results_test['tuned_score'] - eval_results_test['orig_score'])\n",
    "    model_evals.append([model,  round(d_score,2), round(disaccuracy,2)])\n",
    "\n",
    "pd.DataFrame(model_evals, columns=[\"model\",\"d_score\",\"disaccuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate model on titles augmented with author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>d_score</th>\n",
       "      <th>disaccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_embeddings_finetuned</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert_attention_layer0_finetuned</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert_attention_layer5_finetuned</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert_attention_layer11_finetuned</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert_classification_finetuned</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model  d_score  disaccuracy\n",
       "0         bert_embeddings_finetuned    -0.61         80.0\n",
       "1   bert_attention_layer0_finetuned    -0.47         40.0\n",
       "2   bert_attention_layer5_finetuned    -0.57         60.0\n",
       "3  bert_attention_layer11_finetuned    -0.71         80.0\n",
       "4     bert_classification_finetuned    -0.64         80.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_answers = [\"cities\", \"being\", \"pride\", \"rises\", \"punishment\"]\n",
    "corner_titles = [\"a tale of two [MASK] by charles dickens.\",\n",
    "\"the importance of [MASK] earnest by oscar wilde.\",\n",
    "\"[MASK] and prejudice by jane austen.\",\n",
    "\"the sun also [MASK] by ernest hemingway.\",\n",
    "\"crime and [MASK] by fyodor dostoevsky.\"]\n",
    "\n",
    "model_evals = []\n",
    "for model in model_list:\n",
    "    eval_results_corner = evaluate_fine_tuning(model, corner_titles, corner_answers)\n",
    "    disaccuracy = 100*(1-np.mean(eval_results_corner['orig_answer'] == eval_results_corner['tuned_answer']))\n",
    "    d_score = np.mean(eval_results_corner['tuned_score'] - eval_results_corner['orig_score'])\n",
    "    model_evals.append([model,  round(d_score,2), round(disaccuracy,2)])\n",
    "\n",
    "pd.DataFrame(model_evals, columns=[\"model\",\"d_score\",\"disaccuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b0rt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
